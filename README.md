# ğŸ¤– llm_toolkit

**Mini ChatGPT local** - Interface Python para **llama.cpp + Gemma 2B**

## âš¡ Quick Start

```python
# Setup
python llm_toolkit/download.py  # Baixa binÃ¡rio + modelo (~1.6GB)

# Use
from llm_toolkit import gerar_txt
print(gerar_txt("Explique Python em 2 linhas"))
```

## ğŸ¯ Features

âœ… **100% Offline** apÃ³s setup  
âœ… **1 funÃ§Ã£o simples**: `gerar_txt(prompt)`  
âœ… **PortÃ¡til**: funciona em qualquer Windows  
âœ… **RÃ¡pido**: Gemma 2B Q4 (~3GB RAM)

## âš ï¸ Sobre os BinÃ¡rios

**Por que nÃ£o commitamos `bin/` e `models/`?**

- ï¿½ **Tamanho**: Modelo = 1.6GB (GitHub limite: 100MB)
- ğŸ”„ **AtualizaÃ§Ãµes**: llama.cpp muda frequentemente  
- ğŸŒ **Fonte oficial**: Downloads diretos do HuggingFace/GitHub
- ğŸš€ **Performance**: UsuÃ¡rio baixa a versÃ£o mais atual

**SoluÃ§Ã£o**: Script `download.py` automatiza tudo!